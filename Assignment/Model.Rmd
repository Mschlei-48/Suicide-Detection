---
title: "WebApp"
author: "Mishi Makade"
date: "18/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

```{r cars}
library(tidytext)
library(gmodels)
library(dplyr)
library(keras)
library(ggplot2)
library(tm)
library(textclean)
library(e1071)
library(syuzhet)
library(lubridate)
library(reshape2)
library(caret)
library(RCurl)
library(OpenImageR)
library(magick)
library(pdftools)
library(tidyverse)
library(magick)
library(pdftools)
library(tesseract)
```

## Data


```{r }
data=read.csv('C:/Users/Student/Desktop/Suicide_Detection.csv',nrows = 6000) #read in only 6000 rows of the data 
head(data)

```



```{r }
lang=tesseract("eng") #set the language for the OCR
```


# Image Data

```{r }
sui_list=list.files(path="C:/Users/Student/Desktop/Fol/3rd Year 2ND Semester/DataScience2B/Project/New_SuicideQuotes", pattern=".jpg|.png",all.files=T, full.names=T) #list filenames of jpg and png images 

sui_images=c()
for(img in sui_list)
{
  im=image_read(img) #read in images
  sui_images=append(im,sui_images)
}


images=c()
list=list.files(path="C:/Users/Student/Desktop/Fol/3rd Year 2ND Semester/DataScience2B/Project/New_HappyQuotes", pattern=".jpg|.png",all.files=T, full.names=T)
#list filenames of jpg and png images

for(img in list)
{
  im=image_read(img) #read in images
  images=append(im,images)
}
```


# Improve image quality

```{r }
sui_imp=c()
for (i in 1:length(sui_images)) {
      img=image_modulate(sui_images[i], brightness = 200) #increase brightness of images 
      img=image_contrast(img)  #increase contrast of images
      img=image_enhance(img)  #removes noise from the images  
      sui_imp=append(img,sui_imp)
}
imp=c()
for (i in 1:length(images)) {
      img=image_modulate(images[i], brightness = 200)
      img=image_contrast(img)
      img=image_enhance(img)
      imp=append(img,imp)
}
```



# OCR on image data

```{r }
sui_txts=c()
for (i in 1:length(sui_imp)) {
      text=tesseract::ocr(sui_imp[i], engine = lang)  #extracts English texts from the images using ocr
      sui_txts=append(text,sui_txts)
}
txts=c()
for (i in 1:length(imp)) {
      text=tesseract::ocr(imp[i], engine = lang)
      txts=append(text,txts)
}
```


# Combine image data and csv data


```{r }
DF <- as.data.frame(sui_txts)
DF2 <- as.data.frame(txts)

```




```{r }
DF["class"] <- "suicide"
DF2["class"] <- "non-suicide"
```




```{r }
colnames(DF)=c("text","class")
colnames(DF2)=c("text","class")
```

```{r}
data=subset(data,select = -c(X)) #remove the X column
```


```{r}

dataframe=rbind(DF,data) #combine the image data and csv data with rows

```


```{r}
dataframe=rbind(DF2,data) #combine the image data and csv data with rows

```


# Splitting data

```{r}

train=dataframe[1:5000,] #take 80% of the data as train and the rest as test
test=dataframe[5000:6033,]


```


```{r}
View(train)
```






# Cleaning the combined data

```{r}
train_texts=train$text #take the text column from the train data for cleaning
test_texts=test$text   #take the text column from the test data for cleaning
train_corpus=Corpus(VectorSource(train_texts))
test_corpus=Corpus(VectorSource(test_texts))
```




```{r}
train_corpus=tm_map(train_corpus,tolower) #change upperscase letters to lowercase
test_corpus=tm_map(test_corpus,tolower)   #change upperscase letters to lowercase

```


```{r}
train_corpus=tm_map(train_corpus,removeNumbers)  #remove numbers
test_corpus=tm_map(test_corpus,removeNumbers)    #remove numbers

```




```{r}
train_corpus=tm_map(train_corpus,removeWords,stopwords('english'))  #remove stopwords
test_corpus=tm_map(test_corpus,removeWords,stopwords('english'))    #remove stopwords

```




```{r}
train_corpus=tm_map(train_corpus,removePunctuation)  #remove punctuations
test_corpus=tm_map(test_corpus,removePunctuation)    #remove punctuations

```




```{r}
removeSym=function(x) gsub("\n", "  " , x)

train_corpus=tm_map(train_corpus,content_transformer(removeSym))  #remove newline caharacters
test_corpus=tm_map(test_corpus,content_transformer(removeSym))    #remove newline caharacters

```

```{r}
removeSym=function(x) gsub("@", "  " , x)

train_corpus=tm_map(train_corpus,content_transformer(removeSym))  # remove @ characters
test_corpus=tm_map(test_corpus,content_transformer(removeSym))    # remove @ characters

```




```{r}
# remove useless words

removeSym=function(x) gsub("retweeted", "  " , x)

train_corpus=tm_map(train_corpus,content_transformer(removeSym))
test_corpus=tm_map(test_corpus,content_transformer(removeSym))


removeSym=function(x) gsub("twitter", "  " , x)
train_corpus=tm_map(train_corpus,content_transformer(removeSym))
test_corpus=tm_map(test_corpus,content_transformer(removeSym))

removeSym=function(x) gsub("tweet", "  " , x)
train_corpus=tm_map(train_corpus,content_transformer(removeSym))
test_corpus=tm_map(test_corpus,content_transformer(removeSym))


```



```{r}
# remove useless words

removeSym=function(x) gsub("android", "  " , x)
train_corpus=tm_map(train_corpus,content_transformer(removeSym))
test_corpus=tm_map(test_corpus,content_transformer(removeSym))

removeSym=function(x) gsub("jan", "  " , x)

train_corpus=tm_map(train_corpus,content_transformer(removeSym))
test_corpus=tm_map(test_corpus,content_transformer(removeSym))

removeSym=function(x) gsub("quotes", "  " , x)

train_corpus=tm_map(train_corpus,content_transformer(removeSym))
test_corpus=tm_map(test_corpus,content_transformer(removeSym))

removeSym=function(x) gsub("lauvsongs", "  " , x)

train_corpus=tm_map(train_corpus,content_transformer(removeSym))
test_corpus=tm_map(test_corpus,content_transformer(removeSym))

```



```{r}
#remove unicode characers

removeSym=function(x) gsub('[^ -~]', '', x)

train_corpus=tm_map(train_corpus,content_transformer(removeSym))
test_corpus=tm_map(test_corpus,content_transformer(removeSym))

```


```{r}
# remove swearing words

removeSym=function(x) gsub("fuck", '', x)

train_corpus=tm_map(train_corpus,content_transformer(removeSym))

test_corpus=tm_map(test_corpus,content_transformer(removeSym))
```



```{r}
#remove the whitespaces remaining after the cleaning
cleantext=tm_map(train_corpus,stripWhitespace)

```



```{r}
#create a term document matrix
train_tdm=DocumentTermMatrix(train_corpus)
test_tdm=DocumentTermMatrix(test_corpus)
```



# Get the frequency of words before doing the classification

```{r}
FreqWords <- findFreqTerms(train_tdm, 5) #find the 5 most frequent words


Dictionary <- function(x) {
        if( is.character(x) ) {
                return (x)
        }
        stop('x is not a character vector')   #save word list of most frequent terms as a dictionary
}

data_dict <- Dictionary(findFreqTerms(train_tdm, 5))


#add the tdm to the test and train dataset

data_train <- DocumentTermMatrix(train_corpus, list(data_dict)) 
data_test <- DocumentTermMatrix(test_corpus, list(data_dict))



# change frequency  of words to count
convert_counts <- function(x) {
        x <- ifelse(x > 0, 1, 0)
        x <- factor(x, levels = c(0, 1), labels = c("No", "Yes")) 
        return(x)
}


#add count function to train and test dataset
data_train <- apply(data_train, MARGIN = 2, convert_counts)
data_test <- apply(data_test, MARGIN = 2, convert_counts)
```


# Preprocess the class

```{r}
train_labels=as.factor(train$class) #change class to factor
test_labels=as.factor(test$class)   #change class to factor
```

# Train the model

```{r}
data_classifier <- naiveBayes(data_train, train_labels) #train the Naive Bayes model
```


# Get accuracy and predictions

```{r}
data_test_pred <- predict(data_classifier, data_test)  # Get the confusion matrix for the model
CrossTable(data_test_pred, test_labels,
           prop.chisq = TRUE, prop.t = FALSE,
           dnn = c('predicted', 'actual'))
```


```{r}
data_test_pred <- predict(data_classifier, "I want to kill myself. I really do want to kill myself right now. Kill me now. I am suicidal")
```




```{r}
length(data_test_pred)
```




```{r}
test_labels
```



```{r}
c=0
for (i in 1:length(test_labels)) 
{
  if(test_labels[i]==data_test_pred[i]) #check how many were predicted accurately
  {
    c=c+1
  }
    
}

```



```{r}
accuracy=(c/length(test_labels))*100 #get the accuracy of the model
```


```{r}
accuracy
```


# Save model

```{r}
saveRDS(data_classifier,"model.rds") #save the model as an RDS 
```







